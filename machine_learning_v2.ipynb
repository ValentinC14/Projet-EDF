{
 "cells": [
  {
   "source": [
    "## Importation des librairies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.preprocessing\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "from keras.layers import Dense,Dropout,SimpleRNN,LSTM, GRU, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import r2_score\n",
    "from keras import optimizers\n",
    "\n",
    "pd.set_option('max_column', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'une liste des idenfitiants présent dans le dossier dfcomp_vent et d'une liste contenant tout les URL csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'Projet-EDF/dfcomp_vent'\n",
    "URL_ALL = []\n",
    "liste_id = []\n",
    "for name in os.listdir(PATH):\n",
    "    liste_id.append(name.split('_')[1].split('.')[0])\n",
    "    URL_ALL.append(os.path.join(PATH, name))\n",
    "print('Le dossier contient ',len(URL_ALL),'stations différentes avec la température ET le vent')\n",
    "print('_____________________________________________')\n",
    "print('Un exemple des 5 premiers élements de liste_id :',liste_id[:5])\n",
    "print('_____________________________________________')\n",
    "print('Un exemple des 5 premiers élements de URL_ALL :\\n',URL_ALL[:5])\n"
   ]
  },
  {
   "source": [
    "# Preprocessing sur les df comp"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idée ici est de concatener tous les fichiers csv du type comp_X où X est l'identifiant de la station. Chacun de ces csv contient le relevé en température et en vent de la station X à pas de temps horaire sur une durée de quasi 10 mois. Ils sont accompagnées de leur données de référence ( = \"synop\")."
   ]
  },
  {
   "source": [
    "### Exemple d'un df_comp "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(0,len(URL_ALL)) \n",
    "print('Exemple du dataframe pour la station avec comme identifiant : ', liste_id[index])\n",
    "pd.read_csv(URL_ALL[index],  sep=';', index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plusieurs remarques avant de concatener tous les dataframes :\n",
    "- Ici, les seules colonnes qui nous intéressent sont les dates (l'index) ; la température météociel et le vent météociel !\n",
    "- Pour une première approche, on va seulement considérer les dataframe avec aucunes valeurs manquantes sur les données météociel (il reste plus que la moitié des stations)\n",
    "- On va arrondir les données de vent synop à la 3ème décimales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenation_dfcomp(liste_url):\n",
    "    liste_df = [] ### Initialisation de la liste qui permettra de réaliser la concatenation à la fin\n",
    "    \n",
    "    columns_to_drop = ['date','temperature_synop','vent_synop','ind','annee','mois','jour','heure','difference_temperature','difference_vent'] ### les colonnes à enlever\n",
    "    compt_na = 0 ### Un compteur qui va comptabiliser les stations qui ne seront pas concaténer car présentant des valeurs manquantes\n",
    "    liste_sans_na_id = [] ### Liste qui retriendra tout les identifiants des df qui seront concaténés\n",
    "    \n",
    "    for index, url in enumerate(URL_ALL):\n",
    "        \n",
    "        ### Mise en forme du df en accord avec les points précédents\n",
    "        df = pd.read_csv(url, sep=';', index_col = 'Unnamed: 0')\n",
    "        id = int(liste_id[index])\n",
    "        df = df.rename(columns = {'Unnamed: 0':'Datetime'}).drop(columns_to_drop, axis=1)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df['vent_meteociel'] = df['vent_meteociel'].apply(lambda x : round(x,2))\n",
    "        \n",
    "        ### Variable contenant le nombre de valeurs manquantes sur la colonne température météociel et température vent\n",
    "        isna = df.isna().sum().tolist()\n",
    "        \n",
    "        ### On ajoute pour chaque dataframe une colonne de son identifiant, ce qui permettra de le repérer par la suite\n",
    "        id_serie = [id] * len(df)\n",
    "        df['id'] = id_serie\n",
    "\n",
    "        ### On regarde si isna == [0, 0] en d'autres termes, si le df présente aucune valeurs manquantes sur les données météociel\n",
    "        if isna != [0, 0]:\n",
    "            compt_na = compt_na + 1\n",
    "        \n",
    "        else:\n",
    "            liste_sans_na_id.append(id)\n",
    "            liste_df.append(df)\n",
    "    \n",
    "    ### Concaténation des df\n",
    "    df_tot = pd.concat([df for df in liste_df]).reset_index() ### On reset_index car l'index est notre pas de temps et par la suite, c'est mieux d'éviter d'avoir notre pas de temps en index\n",
    "    \n",
    "    return [df_tot, compt_na, liste_sans_na_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot, compt_na, liste_id_sans_na = concatenation_dfcomp(URL_ALL)\n",
    "print('Le df concatené a un shape de ', df_tot.shape)\n",
    "print('_____________________________________________')\n",
    "print('Le nombre de stations avec des valeurs manquantes est ',compt_na,', ce qui fait ',round(compt_na/len(URL_ALL)*100,3),'% de df non utilisé')\n",
    "print('_____________________________________________')\n",
    "print('Exemple dun sample du dftot')\n",
    "df_tot.sample(10)"
   ]
  },
  {
   "source": [
    "# Preprocessing de df map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df_map est la base de données qui fournit les informations relatives à chaque stations (ville, département, altitude, coordonnées GPS ...). Du preprocessing s'impose afin de la mettre dans une forme acceptable. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = pd.read_csv('Projet-EDF/Stations Meteociel 2 - Stations.csv')\n",
    "df_map = df_map[['id Station', 'Coordonnées GPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Convertisseur de coordonées GPS en Degré\n",
    "\n",
    "def gps2deg(coord): \n",
    "    coord1 = str(coord).split('°')\n",
    "    print(coord1)\n",
    "    d = int(coord1[0])\n",
    "    m = int(coord1[1][0:2])\n",
    "    s = int(coord1[1][4:5])\n",
    "    dd = d + m/60 + s/3600\n",
    "    rd = dd/180*np.pi\n",
    "    return rd/np.pi*180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'idée ici est de convertir toutes les coordonées GPS en degré pour la suite (le problème est de prendre en compte la différence Nord/Sud et Est/Ouest)\n",
    "\n",
    "df_split_coord = df_map['Coordonnées GPS'].str.split(expand = True)\n",
    "df_map['lat_mc'] = df_split_coord.loc[:,0].apply(gps2deg)\n",
    "df_map['long_mc'] = df_split_coord.loc[:,2].apply(gps2deg)\n",
    "df_split_coord['S?'] = (df_split_coord.loc[:,1] == 'S') | (df_split_coord.loc[:,1] == 'South')\n",
    "df_split_coord['O?'] = (df_split_coord.loc[:,3] == 'O') | (df_split_coord.loc[:,3] == 'Ouest')\n",
    "for k in range(len(df_meteociel)):\n",
    "    if df_split_coord.loc[k,'S?']:\n",
    "        df_map.loc[k,'lat_mc'] =   - df_map.loc[k,'lat_mc']\n",
    "    elif df_split_coord.loc[k,'O?']:\n",
    "        df_map.loc[k,'long_mc'] =   - df_map.loc[k,'long_mc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectif \n",
    "L'objectif maintenant est de mettre en forme le dataframe précedent telle qu'il soit alimenté dans le futur algorithme d'apprentissage automatique.\n",
    "On souhaite avoir un dataframe de la façon suivante : \n",
    "- chaque ligne correspond à un instant t, donc ici une date avec l'année ; le mois ; le jour et l'heure\n",
    "- chaque colonne correspond à une valeur d'une température ou de vent d'une station spécifique, du type temp_355 qui correspond à la température de la station 355\n",
    "\n",
    "Pour ce faire, on va utiliser un outil de panda qui s'appel \"pivot\" qui est résumé dans l'image suivante :\n",
    "\n",
    "<img src=\"https://tse3.mm.bing.net/th?id=OIP.8RrnZvQQDumdLm9A23C7HwHaDz&pid=Api\">\n",
    "\n",
    "Il faut donc ajouter le nom des colonnes dans le dataframe original pour après appliquer le pivot !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ajout_colonne_nom(df, liste_id):\n",
    "    ### date unique est une variable qui sauvegarde toutes les dates différentes du df\n",
    "    date_unique = df_tot['index'].unique()\n",
    "    \n",
    "    liste_nom_colonne_temp = []\n",
    "    liste_nom_colonne_vent = []\n",
    "    \n",
    "    for ident in liste_id:\n",
    "        N = len(df[df['id']== int(ident)])\n",
    "        \n",
    "        for index in range(N):\n",
    "            liste_nom_colonne_temp.append('temp_{}'.format(ident))\n",
    "            liste_nom_colonne_vent.append('vent_{}'.format(ident))\n",
    "    \n",
    "    df['nom_temp'] = liste_nom_colonne_temp\n",
    "    df['nom_vent'] = liste_nom_colonne_vent\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ajout_colonne_nom(df_tot, liste_id_sans_na)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_pivot(df):\n",
    "    \n",
    "    df_temp = df.pivot(index = 'index', columns = 'nom_temp', values = 'temperature_meteociel')\n",
    "    df_vent = df.pivot(index = 'index', columns = 'nom_vent', values = 'vent_meteociel')\n",
    "    \n",
    "    df_final_ml = pd.merge(df_temp, df_vent, how='outer', on='index')\n",
    "    \n",
    "    return df_final_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ml = double_pivot(df)\n",
    "print('Shape de df_final_ml :',df_final_ml.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, le shape du dataframe est cohérent. En effet : \n",
    "- il y a 360 - 189 = 171 stations dans notre dataframe. Pour chaque station, on a la température et le vent d'ou 171*2 = 342 colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ème temps : EDA sur le df précedemment crée\n",
    "Maintenant qu'on a obtenu le df souhaité, il faut d'abord se préocuper des valeurs manquantes qui ont l'air bien présente\n",
    "On réalise un peu d'EDA pour voir l'importance des NaN !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df_final_ml.isna(), cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le blanc représente les valeurs manquantes au sein du dataframe, déjà quel est son pourcentage ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pourcentage_missing_value = df_final_ml.isna().sum().sum() / (df_final_ml.shape[0]*df_final_ml.shape[1]) * 100\n",
    "print('Il y a donc ', pourcentage_missing_value,'% de valeurs manquantes dans le dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ème temps : s'occuper de ces valeurs manquantes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise une class de scikit-learn récemment crée : \"IterativeImputer\"\n",
    "\n",
    "Cette classe utilise les autres colonnes (features) pour remplacer les valeurs manquantes !\n",
    "\n",
    "Dans notre cas, ceci a du sens car globalement, les variations de température sont relativement proches sur tout le territoire français."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction d'imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_na(df):\n",
    "    ### Création du nouveau dataframe avec les valeurs qui ont été remplacées\n",
    "    imputer = IterativeImputer(random_state = 42)\n",
    "    array_df = df_final_ml.values ### Converti le dataframe en tableau \n",
    "    new_array = imputer.fit_transform(array_df)\n",
    "    df_sans_na = pd.DataFrame(new_array) \n",
    "    \n",
    "    ### Mettre les dates et nom de colonnes de l'ancien sur le nouveau\n",
    "    liste_col = list(df_final_ml.columns)\n",
    "    liste_date = list(df_final_ml.index)\n",
    "    df_sans_na.columns = liste_col\n",
    "    df_sans_na['date'] = liste_date\n",
    "    df_final = df_sans_na.set_index('date')\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "X = impute_na(df_final_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vérification de la sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape de X : ',X.shape)\n",
    "print('_____________________')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(X.isna(), cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainsi, toutes les valeurs ont bien été remplacées comme voulu, le preprocessing est terminé !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création des labels (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dossier qui permettra d'avoir les labels est un excel de Eco2mix RTE, un site qui regroupe toutes les informations de productions, de consommation et d'échange électrique. \n",
    "\n",
    "Nous voulons avoir comme sortie les données de production éolienne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rte = pd.read_excel('Projet-EDF/eCO2mix_RTE_2019-actuel (1).xlsx')\n",
    "df_rte.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque bien les colonnes \"Solaire\", ainsi il faut réaliser les tâches suivantes lors du pre-processing : \n",
    "   - sélectionner seulement les colonnes qui nous intéressent : date ; heure ; eolien ; solaire\n",
    "   - regrouper la date et heure en une colonne de type \"Datetime\" (car Heures est de type object)\n",
    "   - Fusionner le dataframe des données d'entraînements avec ce nouveau dataframe des labels sur les heures pour avoir la production électrique aux mêmes pas de temps que le X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing de y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_rte(df):\n",
    "\n",
    "    df = df.drop(df.tail(1).index, axis=0) # drop last 1 row\n",
    "\n",
    "    ### on garde les colonnes suivantes \n",
    "    columns_to_keep = ['Date','Heures','Eolien']\n",
    "    df = df[columns_to_keep]\n",
    "    \n",
    "    ### Création d'une unique colonne Datatime (on passe en str puis de nouveau en datetime : + simple et rapide)\n",
    "    df['Date_str'] = df['Date'].astype(str)\n",
    "    df['Heures_str'] = df['Heures'].astype(str)\n",
    "    df['Datetime_str'] = df['Date_str'] +' '+ df['Heures_str']\n",
    "\n",
    "    format_string = \"%Y-%m-%d %H:%M:%S\" \n",
    "    df['date'] = df['Datetime_str'].apply(lambda x : datetime.datetime.strptime(x, format_string)) ###strptime passe d'un format string en un datetime\n",
    "    \n",
    "    ### On enelève les colonnes maintenant inutiles\n",
    "    df = df.drop(['Date','Heures','Heures_str','Datetime_str','Date_str'], axis=1)\n",
    "    \n",
    "    ### On renomme les colonnes\n",
    "    list_col = ['eolien','date']\n",
    "    df.columns = list_col\n",
    "\n",
    "    return df\n",
    "\n",
    "y = preprocessing_rte(df_rte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape de y : ', y.shape)\n",
    "print('___________________\\n')\n",
    "display(y.info())\n",
    "print('___________________\\n')\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion des données d'entraînement et les labels. \n",
    " Nous pourrions nous satsifaire du travail précedent et éviter la fusion (surtout si c'est pour spliter de nouveau après). Cependant, il est important de mettre sur le même pas de temps toutes les données. Ainsi, des données manquantes peuvent apparaître. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_X_y(X,y):\n",
    "    X = X.reset_index()\n",
    "    \n",
    "    Xy = pd.merge(X, y, how='left', on='date')\n",
    "    \n",
    "    Xy = Xy.set_index('date')\n",
    "    \n",
    "    return Xy\n",
    "\n",
    "Xy = merge_X_y(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape de Xy : ', Xy.shape)\n",
    "print('___________________\\n')\n",
    "display(Xy.info())\n",
    "print('___________________\\n')\n",
    "print('Nombre de valeurs manquantes : ', Xy.isna().sum().sum())\n",
    "print('___________________\\n')\n",
    "Xy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant l'objectif est de construire un algorithme de ML et/ou DL permettant le point suivant : \n",
    "- On donne à l'algorithme l'historique des données de vent des stations dans toutes la France sur les 24H précédents (entre J et J-1)\n",
    "- L'algo doit prédire le facteur de charge de la production éolienne sur 24h APRÈS (J+1)\n",
    "\n",
    "On doit alors mettre en forme ces données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en forme de X et y à partir de Xy , permettant d'alimenter l'algo de DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mise_en_forme(Xy, pas = 24):\n",
    "    m = len(Xy) ### Nombre de ligne (=exemples)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for index_row in range(pas, m-pas):\n",
    "        X_1_example = Xy.iloc[index_row - pas : index_row, :-2 ]\n",
    "        X_ravel = np.ravel(X_1_example)\n",
    "        X.append(X_ravel)\n",
    "        \n",
    "        y_1_example = Xy.iloc[index_row + pas, -2:].tolist()\n",
    "        y.append(y_1_example)\n",
    "        \n",
    "    X_df = pd.DataFrame(X)\n",
    "    y_df = pd.DataFrame(y)\n",
    "    \n",
    "    return [X_df, y_df]\n",
    "\n",
    "X, y = mise_en_forme(Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape de X : ', X.shape)\n",
    "print('___________________\\n')\n",
    "display(X.info())\n",
    "print('___________________\\n')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape de y : ', y.shape)\n",
    "print('___________________\\n')\n",
    "display(y.info())\n",
    "print('___________________\\n')\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val/Test set\n",
    "\n",
    "Maintenant, on va séparer notre X et y en 3 split, permettant d'entraîner notre modèle sur le train set et l'améliorer sur la validation set pour enfin l'évaluer sur le test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, y, test_size = 0.2, val_size = 0.2):\n",
    "    \n",
    "    scaler = StandardScaler() \n",
    "    \n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size = val_size)\n",
    "\n",
    "    \n",
    "    return [X_train_full, X_train, X_val, X_test, y_train_full, y_train , y_val, y_test]\n",
    "    \n",
    "X_train_full, X_train, X_val, X_test, y_train_full, y_train , y_val, y_test = split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train_full shape : ',X_train_full.shape)\n",
    "print('X_train shape : ',X_train.shape)\n",
    "print('X_val shape : ',X_val.shape)\n",
    "print('X_test shape : ',X_test.shape)\n",
    "print('y_train_full shape : ',y_train_full.shape)\n",
    "print('y_train shape : ',y_train.shape)\n",
    "print('y_val shape : ',y_val.shape)\n",
    "print('y_test shape : ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection de modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence avec des modèles de regression simple pour abouter sur des réseaux de neurones complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "reg = LazyRegressor(verbose = 2, ignore_warnings = False, custom_metric =None)\n",
    "models, predictions = reg.fit(X_train_full, X_test, y_train_full, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "reg = LazyRegressor(verbose = 2, ignore_warnings = False, custom_metric =None)\n",
    "models_w_scale, predictions_w_scale = reg.fit(X_train_full_scale, X_test_scale, y_train_full, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_w_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque alors que, dans ce type de problème, une MLP semble être le meilleur algorithme. \n",
    "On va alors optimiser les hyperparamètre pour avoir le meilleur résultat possible avec le MLP Regressor de Sklearn !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation du MLP Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on va optimiser les paramètres suivant du MLPRegressor : hidden_layer_size et alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'hidden_layer_sizes' : [(128, 64, 32, 16), (64, 32), (1024, )]}\n",
    "reg = GridSearchCV(MLPRegressor(), parameters)\n",
    "\n",
    "reg.fit(X_train_full_scale, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg.score(X_train_full, y_train_full))\n",
    "print(reg.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle de Deep Learning avec Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_simple_sans_dropout():\n",
    "\n",
    "    inputs = keras.Input(shape=(8208,))\n",
    "\n",
    "\n",
    "    x = layers.Dense(1024, activation='relu',kernel_initializer=\"he_normal\")(inputs)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "        \n",
    "    x = layers.Dense(64, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "    \n",
    "    x = layers.Dense(8, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    outputs = layers.Dense(2)(x)\n",
    "    \n",
    "    model = keras.Model(inputs = inputs, outputs = outputs, name='model_simple_sans_dropout')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model_simple_avec_dropout(dropout_rate = 0.2):\n",
    "\n",
    "    inputs = keras.Input(shape=(8208,))\n",
    "\n",
    "\n",
    "    x = layers.Dense(1024, activation='relu',kernel_initializer=\"he_normal\")(inputs)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Dense(256, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "       \n",
    "    x = layers.Dense(64, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = layers.Dense(8, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    outputs = layers.Dense(2)(x)\n",
    "    \n",
    "    model = keras.Model(inputs = inputs, outputs = outputs, name='model_simple_avec_dropout')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model_complexe_sans_dropout():\n",
    "\n",
    "    inputs = keras.Input(shape=(8208,))\n",
    "\n",
    "    x = layers.Dense(2028, activation='relu',kernel_initializer=\"he_normal\")(inputs)\n",
    "\n",
    "    x = layers.Dense(1024, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    x = layers.Dense(512, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    x = layers.Dense(256, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    x = layers.Dense(128, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    x = layers.Dense(64, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "    \n",
    "    x = layers.Dense(8, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    outputs = layers.Dense(2)(x)\n",
    "    \n",
    "    model = keras.Model(inputs = inputs, outputs = outputs, name='model_complexe_sans_dropout')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_model_complexe_avec_dropout(dropout_rate = 0.2):\n",
    "\n",
    "    inputs = keras.Input(shape=(8208,))\n",
    "    \n",
    "    x = layers.Dense(2028, activation='relu',kernel_initializer=\"he_normal\")(inputs)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Dense(1024, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Dense(512, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = layers.Dense(256, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Dense(128, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Dense(8, activation='relu',kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "\n",
    "    outputs = layers.Dense(2)(x)\n",
    "    \n",
    "    model = keras.Model(inputs = inputs, outputs = outputs, name='model_complexe_avec_dropout')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = create_model_simple_sans_dropout()\n",
    "model2 = create_model_simple_avec_dropout()\n",
    "model3 = create_model_complexe_sans_dropout()\n",
    "model4 = create_model_complexe_avec_dropout()\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "    loss=keras.losses.MSE,\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    ")\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_best_model_simple_nodropout_v2.h5\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model1.fit(X_train, y_train, batch_size=32, epochs=60, validation_data=(X_val, y_val), callbacks=[checkpoint_cb,tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train - y_pred_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple_nodrop = keras.models.load_model(\"my_model_EDF_complexe_avec_dropout.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val = pd.DataFrame(model1.predict(X_val))\n",
    "y_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(y_val - y_pred_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python390jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}